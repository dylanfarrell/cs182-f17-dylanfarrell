{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 182: Artificial Intelligence\n",
    "# Assignment 3: Markov Decision Processes and Reinforcement Learning\n",
    "\n",
    "* Fall 2017\n",
    "* Due: **Thursday**, October 19, 2016\n",
    "\n",
    "In this assignment, you will implement sequence decision making algorithms and apply these to a gridworld and to Pacman. Note: We will use the Pacman framework developed at Berkeley. This framework is used worldwide to teach AI, therefore it is very important that you DO NOT publish your solutions online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Computational Assignment\n",
    "\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/cs188websitecontent/projects/release/reinforcement/v1/001/capsule.png\" />\n",
    "\n",
    "### Pacman (22 points)\n",
    "\n",
    "Follow the instructions at:\n",
    "\n",
    "http://ai.berkeley.edu/reinforcement.html\n",
    "\n",
    "The page includes questions requiring implementation of sequential decision making and reinforcement algorithms we studied in class. [We will be using the Berkeley autograding to grade this part of the problem set. Make sure you test with the **autograder**!]\n",
    "\n",
    "For this assignment you are only required to do **Questions 1-7**. Question 8 is quite interesting though, so we recommend it if you have time and are interested in machine learning (for 1 extra credit point for a 3/3). Please put your answers in the files as specified in the instructions in the reinforcement folder as provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "The framework for MDPs and reinforcement learning in this assignment is the same as the one discussed in class. However the reward specification is different than the one given in AIMA. While AIMA provides a helpful guide to this topic, if you want further notes on in the style of the assignment consult the text _Reinforcement Learning_ by Sutton & Barto available free at https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing (4 points)\n",
    "\n",
    "We will be solving the classic knapsack problem using Simulated Annealing (SA). Given some items, each item consisting of a weight and value, we want to select the items so as to maximize the total value while staying under some weight limit. Inside simulated_annealing.py is the starter code comparing a greedy solution to your SA solution. The function simulated_annealing should return a list of values your algorithm has accepted (value meaning the total value of your chosen items). Running the file will generate a plot comparing your SA values to the greedy solution. Submit your code and in your written homework attach the plot as well as the total value, weight, and contents of the bag of your best solution (similar to what's printed in the starter code for the greedy solution).\n",
    "\n",
    "Hint: You should beat the greedy solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "The python file requires that you have numpy and matplotlib installed. These are standard libraries in the Anaconda installation of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Assignment \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes (5 points)\n",
    "\n",
    "Annie is a 5-year old girl who *loves* eating candy and is ambivalent regarding vegetables. She can either choose to eat candy (Hershey's, Skittles, Peanut Butter Cups) or eat vegetables during every meal. Eating candy gives her +10 in happiness points, while eating vegetables only gives her +4 happiness points. But if she eats too much candy while sick, her teeth will all fall out (she won't be able to eat any more). Annie will be in one of three states: healthy, sick, and toothless. Eating candy tends to make Annie sick, while eating vegetables tends to keep Annie healthy. If she eats too much candy, she'll be toothless and won't eat anything else. The transitions are shown in the table below.\n",
    "\n",
    "\n",
    "|Health condition | Candy or Vegetables? |  Next condition |  Probability |\n",
    "|------|-------|------|----------------------|\n",
    "| healthy | vegetables | healthy | 1 |\n",
    "|heatlhy | candy | healthy | 1/4| \n",
    "|healthy | candy | sick | 3/4|\n",
    "|sick | vegetables | healthy | 1/4|\n",
    "|sick | vegetables | sick | 3/4|\n",
    "| sick | candy | sick | 7/8 |\n",
    "| sick | candy | toothless | 1/8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (2 points)\n",
    "\n",
    "&nbsp;&nbsp;a. Model this problem as a Markov Decision Process: Formally specify the states, actions, transition function and reward function.\n",
    "\n",
    "&nbsp;&nbsp;b. Write down the Value function $V(s)$ for this problem in all possible states. Calculate by hand the values of $V(s)$ assuming a policy $\\pi_1$ in which Annie always eats candy and $\\pi_2$ in which Annie always eats vegetables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (1 point)\n",
    "\n",
    "Start with a policy in which Annie always eats candy no matter what the her health condition is. Simulate the first two iterations of the *policy iteration* algorithm. Show how the policy evolves as you run the algorithm. What is the policy after the third iteration? For this question assume a discount factor of 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (2 points)\n",
    "Which of the following statements are true for an MDP? Select all that apply and briefly explain why.\n",
    "\n",
    "1. If one is using value iteration and the values have converged, the policy must have converged as well.\n",
    "2. Expectimax will generally run in the same amount of time as value iteration on a given MDP.\n",
    "3. For an infinite horizon MDP with a finite number of states and actions and with a discount factor that satisfies 0 < $\\gamma$ <= 1, policy iteration is guaranteed to converge.\n",
    "4. There may be more than one optimal value function.\n",
    "5. There may be more than one optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (1 point)\n",
    "In class we learned a couple of temporal-difference techniques for reinforcement learning. Now we'd like to take the next step (literally). Suppose we take two steps and get the state/action sequence s-a-s'-a'-s''. Write a temporal-difference update equation in terms of $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 (2 points)\n",
    "\n",
    "Consider the following deterministic Transition/Reward Model for an MDP with states (S1, S2, S3, S4, S5, S6) and actions (A1, A2, A3):\n",
    "\n",
    "| From | Action | To | Reward |\n",
    "|------|--------|----|--------|\n",
    "| S1   | A3     | S2 | 3      |\n",
    "| S2   | A1     | S1 | 2      |\n",
    "| S2   | A2     | S3 | 1      |\n",
    "| S3   | A1     | S4 | 2      |\n",
    "| S3   | A2     | S5 | 10     |\n",
    "| S4   | A3     | S3 | 5      |\n",
    "| S5   | A1     | S6 | 7      |\n",
    "| S6   | A3     | S5 | 2      |\n",
    "\n",
    "&nbsp;&nbsp;a. Suppose we start in state S3. We run Q-learning on this MDP using a greedy policy (always choose action with best Q-value). Ties are given to the action with the lower number (A1 > A2, etc). Assume $\\alpha=0.5$ and $\\gamma=0.9$. What are the first 4 (state, action) pairs visited?\n",
    "\n",
    "&nbsp;&nbsp;b. What is wrong with this algorithm and how can we fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6 (4 points)\n",
    "Consider the following Transition/Reward Model for an MDP with states (S1, S2, S3, S4, S5) and actions (A1, A2):\n",
    "\n",
    "| From | Action | To | Reward | Probability |\n",
    "|------|--------|----|--------|-------------|\n",
    "| S1   | A1     | S2 | 0      | 0.5         |\n",
    "| S1   | A1     | S3 | 0      | 0.5         |\n",
    "| S1   | A2     | S2 | 0      | 0.25        |\n",
    "| S1   | A2     | S3 | 0      | 0.75        |\n",
    "| S2   | A1     | S4 | 6      | 1.0         |\n",
    "| S2   | A2     | S4 | 12     | 0.5         |\n",
    "| S2   | A2     | S5 | 4      | 0.5         |\n",
    "| S3   | A1     | S5 | 16     | 1.0         |\n",
    "\n",
    "(S4, S5) are terminal states with no valid actions. Assume $\\gamma=1.0$\n",
    "\n",
    "&nbsp;&nbsp;a. Compute $Q^*(s,a)$\n",
    "\n",
    "&nbsp;&nbsp;b. Compute the optimal policy\n",
    "\n",
    "&nbsp;&nbsp;c. Initialize Q to 0 for all (state, action) pairs for q-learning. With $\\alpha = 0.5$, compute Q after seeing: \n",
    "\n",
    "    [(S1,A1,S2) -> (S2,A2,S4)] \n",
    "    [(S1,A2,S3) -> (S3,A1,S5)]\n",
    "    [(S1,A2,S2) -> (S2,A1,S4)]\n",
    "\n",
    "&nbsp;&nbsp;d. Notice that whatever our first action is, the rewards are 0. When we're q-learning regardless of what first action we take, we won't gain  much information. Suppose we have an MDP where states can be ordered $S1, S2, S3, ...$ such that there exists no transition $S_i -> S_j$ for $i>j$. Let's redefine our rewards model to be $$\\hat{R}(s,a,s') = Q^*(s,a) - \\gamma V^*(s')$$ \n",
    "\n",
    "so that now our first action is taking into account future rewards. Prove that $\\hat{Q}^*(s,a)$ is unchanged, i.e. $\\hat{Q}^*(s,a) = Q^*(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Beam Search (2 points)\n",
    "\n",
    "Ant colony optimization is an optimization technique that was inspired by the foraging behavior of real ant colonies.  When searching for food, ants initially explore the area surrounding their nest in a random manner. Ants communicate indirectly by means of chemical pheromone trails, which help them find the shortest paths between their nest and the nearest food. While moving, they leave a trail of chemical pheromone behind them. Once an ant finds food, they vary the amount of pheromone they leave depending on the quality and quantity of food. This indirect communication between ants via pheromone trails enables them to find the shortest paths between their nest and the food sources.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Knapsack_ants.svg/2000px-Knapsack_ants.svg.png\" style=\"width:20%\"/>\n",
    "\n",
    "Describe (in words) how the ant colony optimization problem can be modeled through a local beam search algorithm. Indicate what $k$ stands for and how the ants find the $k$-best successors for each iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
